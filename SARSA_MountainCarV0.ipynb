{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2y1Jo9bH4nlA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "envt = gym.make('MountainCar-v0')"
      ],
      "metadata": {
        "id": "ctCekZBA4tM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 200\n",
        "rg = np.random.RandomState(seed)\n",
        "\n",
        "# Epsilon greedy\n",
        "def choose_action_epsilon(Q, state, epsilon, rg=rg):\n",
        "    if not np.any(Q[state]) or rg.rand() < epsilon:\n",
        "        return rg.randint(0, 3) ## three actions are posible for Mountain car (0,1,2)\n",
        "    else:\n",
        "        return np.argmax(Q[state])\n"
      ],
      "metadata": {
        "id": "23yGhDjU4vZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#discretize the spaces\n",
        "pose_discrete = np.linspace(-1.2, 0.6,36)\n",
        "vel_discrete = np.linspace(-0.07 ,0.07,25)\n",
        "\n",
        "def discrete_states(obs):\n",
        "    cartX, cartXdot = obs\n",
        "    cartX = int(np.digitize(cartX, pose_discrete))\n",
        "    cartXdot = int(np.digitize(cartXdot, vel_discrete ))\n",
        "\n",
        "    return (cartX, cartXdot)"
      ],
      "metadata": {
        "id": "8vtLUe924yb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Q-values\n",
        "Q = {}\n",
        "for i in range(19):\n",
        "    for j in range(15):\n",
        "                Q[(i, j)] = np.random.uniform(-0.5,0,size = 3)# to promote random action"
      ],
      "metadata": {
        "id": "r8X_cKrw44D6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##SARSA\n",
        "\n",
        "def sarsa(envt, Q, epsilon_input, alpha_input, rg, gamma = 0.99, choose_action = choose_action_epsilon):\n",
        "    episodes = 30000 #defined\n",
        "    decay=0.9999#defined\n",
        "    epsilon_start=1#defined\n",
        "    episode_rewards = np.zeros(episodes)\n",
        "    mean_score_l100 =[]\n",
        "    steps_to_completion = np.zeros(episodes)\n",
        "    eps = epsilon_start\n",
        "    alp = alpha_input\n",
        "    rg =rg\n",
        "    list_of_tuples=[]\n",
        "    for ep in range(episodes):\n",
        "        tot_reward, steps = 0, 0\n",
        "\n",
        "        # Reset environment\n",
        "        obs,_ = envt.reset()\n",
        "        ##descrete state\n",
        "        state = discrete_states(obs)\n",
        "        #rand = np.random.random()\n",
        "        action = choose_action_epsilon(Q, state, eps)\n",
        "        done = False\n",
        "        truncated =False\n",
        "        while not done and not truncated:\n",
        "            obs_, reward, done,truncated, _ = envt.step(action)\n",
        "            position, velocity = obs_\n",
        "            state_next = discrete_states(obs_)\n",
        "            rewards=reward\n",
        "            if position>-0.4   and velocity>0.5:\n",
        "              rewards=rewards+position+15*velocity\n",
        "            #case1\n",
        "            if position>0.4 and velocity>0.6:\n",
        "              rewards=rewards + 0.7\n",
        "            if state_next in list_of_tuples:\n",
        "              rewards=rewards\n",
        "            else:\n",
        "               list_of_tuples.append(state_next)\n",
        "               rewards+=0.1\n",
        "\n",
        "            action_next = choose_action_epsilon(Q, state_next, eps, rg)\n",
        "            # update equation\n",
        "            Q[state][action] = Q[state][action] + alp * (rewards + gamma * Q[state_next][action_next] - Q[state][action])\n",
        "            tot_reward += reward\n",
        "            steps += 1\n",
        "            state, action = state_next, action_next\n",
        "            eps=max(epsilon_input,decay*eps)\n",
        "        episode_rewards[ep]= tot_reward\n",
        "        steps_to_completion[ep] = steps\n",
        "        if ep>=99:\n",
        "          mean_score_l100.append(np.mean(episode_rewards[ep-99:ep]))\n",
        "        if ep>510:\n",
        "          mean_score = np.mean(episode_rewards[ep-499:ep])\n",
        "          if mean_score>=-140:\n",
        "            print(\"Avg score above -140 for last 500 episode\")\n",
        "    mean_final = np.mean(episode_rewards[-1000:])\n",
        "    return Q, episode_rewards, steps_to_completion, mean_final, mean_score_l100"
      ],
      "metadata": {
        "id": "OdLiwjZ647ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test Run\n",
        "epsilon_input=0.06\n",
        "alpha_input=0.13\n",
        "\n",
        "rg = np.random.RandomState(40)\n",
        "Q, rewards, steps, avg_reward, rewards_100 = sarsa(envt, Q, epsilon_input, alpha_input, rg)\n",
        "print(f'avg_reward for last 1000 episode is {avg_reward}')"
      ],
      "metadata": {
        "id": "Dbqe5EDf5Ddb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists to store results across experiments and seeds\n",
        "all_rewards = []\n",
        "all_steps = []\n",
        "num_seeds = 5\n",
        "epsilon_alpha_tuning = np.array([[0.03, 0.1],[0.01, 0.15],[0.06, 0.08]])\n",
        "mean_rewards_per_epsilon = []\n",
        "std_rewards_per_epsilon = []\n",
        "mean_rewards_per_epsilon_100 = []\n",
        "std_rewards_per_epsilon_100 = []\n",
        "\n",
        "seed_rewards_ = {}\n",
        "avg_reward_exp_ = {}\n",
        "Q_stored_ ={}\n",
        "steps_stored_ = {}\n",
        "seed_rewards_100_ = {}\n",
        "mean_seed_rewards={}\n",
        "mean_seed_rewards_100={}\n",
        "regret={}\n",
        "plt.style.use('seaborn-v0_8-whitegrid')  # Using the correct seaborn style\n",
        "sns.set_style(\"whitegrid\")  # Ensures consistency\n",
        "\n",
        "plt.rcParams.update({\n",
        "    'grid.alpha': 0.5,\n",
        "    'grid.linestyle': '--',\n",
        "    'figure.figsize': (13, 6)\n",
        "})\n",
        "\n",
        "colors = plt.cm.plasma(np.linspace(0, 1, len(epsilon_alpha_tuning)))\n",
        "\n",
        "for exp_idx, exp in enumerate(epsilon_alpha_tuning):  # Use enumerate for safer indexing\n",
        "    print(f\"\\nStarting Experiment {exp_idx+1} with ε={exp[0]}, α={exp[1]}\")\n",
        "\n",
        "    # Initialize lists to store results for each seed in this experiment\n",
        "    seed_rewards = []\n",
        "    avg_reward_exp=[]\n",
        "    Q_stored = []\n",
        "    steps_stored = []\n",
        "    seed_rewards_100 = []\n",
        "\n",
        "    for seed in range(num_seeds):\n",
        "        print(f\"  Running seed {seed+1}\")\n",
        "\n",
        "        # Initialize Q-table (use j instead of i to avoid conflict)\n",
        "        Q = {}\n",
        "        for i in range(37):\n",
        "          for j in range(26):\n",
        "            Q[(i, j)] = np.random.uniform(-1,1,size = 3)\n",
        "\n",
        "        # Run SARSA with the current seed\n",
        "        rg = np.random.RandomState(seed+40)\n",
        "        epsilon_start = exp[0]\n",
        "        alpha_input = exp[1]\n",
        "        Q, rewards, steps, avg_last_1000, rewards_100 = sarsa(envt, Q, epsilon_start, alpha_input, rg)\n",
        "\n",
        "        seed_rewards.append(rewards)\n",
        "        avg_reward_exp.append(avg_last_1000)\n",
        "        Q_stored.append(Q)\n",
        "        steps_stored.append(steps)\n",
        "        seed_rewards_100.append(rewards_100)\n",
        "\n",
        "    # Convert to numpy arrays for easier calculations\n",
        "    seed_rewards_[exp_idx] = np.array(seed_rewards)  # shape: (num_seeds, num_episodes)\n",
        "    avg_reward_exp_[exp_idx] = np.array(avg_reward_exp)\n",
        "    Q_stored_[exp_idx] = np.array(Q_stored)\n",
        "    steps_stored_[exp_idx] = np.array(steps_stored)\n",
        "    seed_rewards_100_[exp_idx] = np.array(seed_rewards_100)\n",
        "\n",
        "\n",
        "    mean_avg = np.mean(avg_reward_exp)\n",
        "    print(f'mean accross 5 seeds of last 1000 episode is {mean_avg}')\n",
        "    ###############################################################################################\n",
        "    # Calculate mean across seeds for this epsilon\n",
        "    mean_seed_rewards[exp_idx] = np.mean(seed_rewards_[exp_idx], axis=0)\n",
        "    mean_rewards_per_epsilon.append(mean_seed_rewards[exp_idx])###############\n",
        "    std_rewards = np.std(seed_rewards_[exp_idx], axis=0)\n",
        "    std_rewards_per_epsilon.append(std_rewards)##############################\n",
        "    ################################################################################################\n",
        "    # Calculate mean across seeds for this epsilon for the reward calculated as avg of every 100 episides\n",
        "    mean_seed_rewards_100[exp_idx] = np.mean(seed_rewards_100_[exp_idx], axis=0)\n",
        "    mean_rewards_per_epsilon_100.append(mean_seed_rewards_100[exp_idx])#######\n",
        "    std_rewards_100 = np.std(seed_rewards_100_[exp_idx], axis=0)\n",
        "    std_rewards_per_epsilon_100.append(std_rewards_100)#######################\n",
        "    ################################################################################################\n",
        "    regret[exp_idx]=np.sum(-110-np.array(mean_seed_rewards[exp_idx]))\n",
        "    eps, alpha = exp\n",
        "    print(f'Calculated Regret for epsilon ={eps} & alpha ={alpha} = {regret[exp_idx]}')\n",
        "\n",
        "    ##single plot\n",
        "\n",
        "    # Define color map (consistent across all plots)\n",
        "    plt.figure()\n",
        "    eps, alpha = exp\n",
        "    plt.plot(mean_seed_rewards_100[exp_idx],label=f'ε={eps}, α={alpha}',color=colors[exp_idx])\n",
        "    plt.fill_between(range(len(mean_seed_rewards_100[exp_idx])),\n",
        "                     mean_seed_rewards_100[exp_idx] - std_rewards_100,\n",
        "                     mean_seed_rewards_100[exp_idx] + std_rewards_100,color=colors[exp_idx],\n",
        "                     alpha=0.1, linewidth=0)\n",
        "\n",
        "\n",
        "            # Set consistent axes and labels\n",
        "    plt.ylim(-200, -110)  # Fixed y-axis range\n",
        "    plt.title(f'SARSA Performance Moutain car means reward across 5 seeds (moving avg of 100 used) (ε={eps}, α={alpha}, {num_seeds} seeds)')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Mean Reward')\n",
        "\n",
        "        # Standardized legend and grid\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1))\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Plotting (same as before)\n",
        "plt.figure()\n",
        "\n",
        "position_=epsilon_alpha_tuning.shape[0]###################################\n",
        "\n",
        "for g in position_:\n",
        "  eps, alpha = epsilon_alpha_tuning[g]\n",
        "  mean_rewards=mean_rewards_per_epsilon_100[g]\n",
        "  std_rewards=std_rewards_per_epsilon_100[g]\n",
        "  colorss=colors[g]\n",
        "  plt.plot(mean_rewards, label=f'ε={eps}', color=colorss, linewidth=1)\n",
        "  plt.fill_between(range(len(mean_rewards)),\n",
        "                   mean_rewards - std_rewards,\n",
        "                   mean_rewards + std_rewards,\n",
        "                   color=colorss, alpha=0.1, linewidth=0)  # Light transparency\n",
        "\n",
        "plt.ylim(-200, -110)  # Fixed y-axis range\n",
        "plt.title(f'SARSA Performance MountainCar V0: Hyperparameters (epsilon & alpha)')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean Reward')\n",
        "\n",
        "    # Standardized legend and grid\n",
        "plt.legend(bbox_to_anchor=(1.05, 1))\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wGq2F8CR5Klq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}