{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANSqVW7V7gHx"
      },
      "outputs": [],
      "source": [
        "##dependencies\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "envt = gym.make('CartPole-v1')"
      ],
      "metadata": {
        "id": "yfW0ghnA7kYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "seed = 122\n",
        "rg = np.random.RandomState(seed)\n",
        "\n",
        "# Epsilon greedy\n",
        "def choose_action_epsilon(Q, state, epsilon, rg=rg):\n",
        "    if not np.any(Q[state]) or rg.rand() < epsilon:\n",
        "        return rg.randint(0, 2)\n",
        "    else:\n",
        "        return np.argmax(Q[state])\n"
      ],
      "metadata": {
        "id": "wzZpabMR7kVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#discretize the spaces\n",
        "discrete_factor = 50\n",
        "theta_discrete = np.linspace(-0.20943951, 0.20943951, discrete_factor)\n",
        "theta_dot_discrete = np.linspace(-2.0, 2.0, discrete_factor//5)\n",
        "pose_discrete = np.linspace(-2.4, 2.4, discrete_factor)\n",
        "vel_discrete = np.linspace(-2.0 ,2.0, discrete_factor//5)\n",
        "\n",
        "def discrete_states(obs):\n",
        "    cartX, cartXdot, cartTheta, cartThetadot = obs\n",
        "    cartX = int(np.digitize(cartX, pose_discrete))\n",
        "    cartXdot = int(np.digitize(cartXdot, vel_discrete ))\n",
        "    cartTheta = int(np.digitize(cartTheta, theta_discrete))\n",
        "    cartThetadot = int(np.digitize(cartThetadot, theta_dot_discrete))\n",
        "\n",
        "    return (cartX, cartXdot, cartTheta, cartThetadot)"
      ],
      "metadata": {
        "id": "OdKY5iBn7kTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Q-values\n",
        "Q = {}\n",
        "for i in range(discrete_factor+1):\n",
        "    for j in range(discrete_factor//5+1):\n",
        "        for k in range(discrete_factor+1):\n",
        "            for l in range(discrete_factor//5+1):\n",
        "                Q[(i, j, k, l)] = np.random.uniform(0,1,size = 2)"
      ],
      "metadata": {
        "id": "YnkQP2QD7kQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##SARSA\n",
        "def sarsa(envt, Q, epsilon_input, alpha_input,rg, gamma = 0.99, choose_action = choose_action_epsilon):\n",
        "    episodes = 22000\n",
        "    episode_rewards = np.zeros(episodes)\n",
        "    steps_to_completion = np.zeros(episodes)\n",
        "    mean_score_l100=[]\n",
        "    eps = epsilon_input\n",
        "    alp = alpha_input\n",
        "    rg=rg\n",
        "    for ep in range(episodes):\n",
        "        tot_reward, steps = 0, 0\n",
        "\n",
        "        # Reset environment\n",
        "        obs,_ = envt.reset()\n",
        "        ##descrete state\n",
        "        state = discrete_states(obs)\n",
        "        #rand = np.random.random()\n",
        "        action = choose_action_epsilon(Q, state, eps)\n",
        "        done = False\n",
        "        while not done:\n",
        "            obs_, reward, done,_, _ = envt.step(action)\n",
        "\n",
        "            state_next = discrete_states(obs_)\n",
        "            action_next = choose_action_epsilon(Q, state_next, eps,rg)\n",
        "            # update equation\n",
        "            Q[state][action] = Q[state][action] + alp * (reward + gamma * Q[state_next][action_next] - Q[state][action])\n",
        "            tot_reward += reward\n",
        "            steps += 1\n",
        "            state, action = state_next, action_next\n",
        "\n",
        "        episode_rewards[ep]= tot_reward\n",
        "        steps_to_completion[ep] = steps\n",
        "        if ep>=99:\n",
        "          mean_score_l100.append(np.mean(episode_rewards[ep-99:ep]))\n",
        "        if ep >510:\n",
        "            mean_score = np.mean(episode_rewards[ep-499:ep])\n",
        "            if mean_score%200==0:\n",
        "                print(\"Avg score above 200 for last 500 episode\")\n",
        "    mean_final = np.mean(episode_rewards)\n",
        "    return Q, episode_rewards, steps_to_completion, mean_final, mean_score_l100"
      ],
      "metadata": {
        "id": "smDoCO-H7kJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists to store results across experiments and seeds\n",
        "all_rewards = []\n",
        "all_steps = []\n",
        "num_seeds = 5\n",
        "epsilon_alpha_tuning = np.array([[0.08, 0.1],[0.08, 0.3],[0.1, 0.1],[0.1, 0.3]])\n",
        "mean_rewards_per_epsilon = []\n",
        "std_rewards_per_epsilon = []\n",
        "mean_rewards_per_epsilon_100 = []\n",
        "std_rewards_per_epsilon_100 = []\n",
        "regret={}\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')  # Using the correct seaborn style\n",
        "sns.set_style(\"whitegrid\")  # Ensures consistency\n",
        "\n",
        "plt.rcParams.update({\n",
        "    'grid.alpha': 0.5,\n",
        "    'grid.linestyle': '--',\n",
        "    'figure.figsize': (13, 6)\n",
        "})\n",
        "colors = plt.cm.plasma(np.linspace(0, 1, len(epsilon_alpha_tuning)))\n",
        "for exp_idx, exp in enumerate(epsilon_alpha_tuning):  # Use enumerate for safer indexing\n",
        "    print(f\"\\nStarting Experiment {exp_idx+1} with ε={exp[0]}, α={exp[1]}\")\n",
        "\n",
        "    # Initialize lists to store results for each seed in this experiment\n",
        "    seed_rewards = []\n",
        "    avg_reward_exp=[]\n",
        "    seed_rewards_100 = []\n",
        "\n",
        "    for seed in range(num_seeds):\n",
        "        print(f\"  Running seed {seed+1}\")\n",
        "\n",
        "        # Initialize Q-table (use j instead of i to avoid conflict)\n",
        "        Q = {}\n",
        "        for j in range(discrete_factor + 1):  # Changed i to j\n",
        "            for k in range(discrete_factor // 5 + 1):\n",
        "                for l in range(discrete_factor + 1):\n",
        "                    for m in range(discrete_factor // 5 + 1):\n",
        "                        Q[(j, k, l, m)] = np.random.uniform(0, 1, size=2)\n",
        "\n",
        "        # Run SARSA with the current seed\n",
        "        rg = np.random.RandomState(seed+40)\n",
        "        epsilon_start = exp[0]\n",
        "        alpha_input = exp[1]\n",
        "        Q, rewards, steps, avg, rewards_100 = sarsa(envt, Q, epsilon_start, alpha_input,rg)\n",
        "\n",
        "        seed_rewards.append(rewards)\n",
        "        avg_reward_exp.append(avg)\n",
        "        seed_rewards_100.append(rewards_100)\n",
        "\n",
        "\n",
        "    # Convert to numpy arrays for easier calculations\n",
        "    seed_rewards = np.array(seed_rewards)  # shape: (num_seeds, num_episodes)\n",
        "    avg_reward_exp = np.array(avg_reward_exp)\n",
        "    seed_rewards_100 = np.array(seed_rewards_100)\n",
        "\n",
        "    # Calculate mean across seeds for this epsilon\n",
        "    mean_avg = np.mean(avg_reward_exp)\n",
        "    print(mean_avg)\n",
        "    mean_rewards = np.mean(seed_rewards, axis=0)\n",
        "    mean_rewards_per_epsilon.append(mean_rewards)\n",
        "    std_rewards = np.std(seed_rewards, axis=0)\n",
        "    std_rewards_per_epsilon.append(std_rewards)\n",
        "    mean_rewards_100 = np.mean(seed_rewards_100, axis=0)\n",
        "    mean_rewards_per_epsilon_100.append(mean_rewards_100)\n",
        "    std_rewards_100 = np.std(seed_rewards_100, axis=0)\n",
        "    std_rewards_per_epsilon_100.append(std_rewards_100)\n",
        "\n",
        "    regret[exp_idx]=np.sum(475-np.array(mean_rewards))\n",
        "    eps, alpha = exp\n",
        "    print(f'Calculated Regret for epsilon ={eps} & alpha ={alpha} = {regret[exp_idx]}')\n",
        "\n",
        "    plt.figure()\n",
        "    eps, alpha = exp\n",
        "    plt.plot(mean_rewards_100,label=f'ε={eps}, α={alpha}',color=colors[exp_idx])\n",
        "    plt.fill_between(range(len(mean_rewards_100)),\n",
        "                     mean_rewards_100 - std_rewards_100,\n",
        "                     mean_rewards_100 + std_rewards_100,color=colors[exp_idx],\n",
        "                     alpha=0.1, linewidth=0)\n",
        "\n",
        "\n",
        "            # Set consistent axes and labels\n",
        "    plt.ylim(0, 550)  # Fixed y-axis range\n",
        "    plt.title(f'SARSA Performance means reward across 5 seeds (moving avg of 100 used) (ε={eps}, α={alpha}, {num_seeds} seeds)')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Mean Reward')\n",
        "\n",
        "        # Standardized legend and grid\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1))\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Plotting (same as before)\n",
        "plt.figure()\n",
        "\n",
        "position_=[0,1,2,3]\n",
        "\n",
        "for g in position_:\n",
        "  eps, alpha = epsilon_alpha_tuning[g]\n",
        "  mean_rewards=mean_rewards_per_epsilon_100[g]\n",
        "  std_rewards=std_rewards_per_epsilon_100[g]\n",
        "  colorss=colors[g]\n",
        "  plt.plot(mean_rewards, label=f'ε={eps}', color=colorss, linewidth=1)\n",
        "  plt.fill_between(range(len(mean_rewards_100)),\n",
        "                   mean_rewards - std_rewards,\n",
        "                   mean_rewards + std_rewards,\n",
        "                   color=colorss, alpha=0.1, linewidth=0)  # Light transparency\n",
        "\n",
        "plt.ylim(0, 550)  # Fixed y-axis range\n",
        "plt.title(f'SARSA Performance Cartpole-V1: Hyperparameters (epsilon & alpha)')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean Reward')\n",
        "\n",
        "    # Standardized legend and grid\n",
        "plt.legend(bbox_to_anchor=(1.05, 1))\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RWtx6Xot7kGy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}